---
description: 
globs: 
alwaysApply: false
---
# Multi-User AI Chat with Vercel AI SDK & Convex

A complete implementation guide for building an efficient streaming AI chat application with multi-user real-time capabilities.

NOTE:  For AI models, we are already using openrouter

## Table of Contents
1. [Project Setup](mdc:#project-setup)
2. [Convex Schema & Backend](mdc:#convex-schema--backend)
3. [Frontend Implementation](mdc:#frontend-implementation)
4. [Streaming Optimization](mdc:#streaming-optimization)
5. [Multi-User Features](mdc:#multi-user-features)
6. [Deployment & Scaling](mdc:#deployment--scaling)

## Project Setup

### 1. Initialize Next.js Project
```bash
npx create-next-app@latest ai-chat-app --typescript --tailwind --eslint
cd ai-chat-app
```

### 2. Install Dependencies
```bash
# AI SDK
npm install ai @ai-sdk/openai

# Convex
npm install convex
npx convex dev

# UI Components (optional)
npm install @radix-ui/react-avatar @radix-ui/react-scroll-area
npm install lucide-react
```

### 3. Environment Variables
Create `.env.local`:
```env
OPENAI_API_KEY=your_openai_api_key
CONVEX_DEPLOYMENT=your_convex_deployment_url
NEXT_PUBLIC_CONVEX_URL=your_convex_url
```

## Convex Schema & Backend

### 1. Database Schema
```typescript
// convex/schema.ts
import { defineSchema, defineTable } from "convex/server";
import { v } from "convex/values";

export default defineSchema({
  chats: defineTable({
    name: v.string(),
    createdAt: v.number(),
    participants: v.array(v.id("users")),
  }).index("by_creation_time", ["createdAt"]),

  messages: defineTable({
    chatId: v.id("chats"),
    content: v.string(),
    role: v.union(v.literal("user"), v.literal("assistant")),
    userId: v.optional(v.id("users")),
    status: v.union(v.literal("complete"), v.literal("streaming"), v.literal("error")),
    streamingForUser: v.optional(v.id("users")),
    createdAt: v.number(),
    updatedAt: v.number(),
  })
    .index("by_chat", ["chatId", "createdAt"])
    .index("by_status", ["status"]),

  users: defineTable({
    name: v.string(),
    email: v.string(),
    avatar: v.optional(v.string()),
  }).index("by_email", ["email"]),

  presence: defineTable({
    chatId: v.id("chats"),
    userId: v.id("users"),
    status: v.union(v.literal("online"), v.literal("typing"), v.literal("away")),
    lastSeen: v.number(),
  })
    .index("by_chat", ["chatId"])
    .index("by_user", ["userId"]),
});
```

### 2. Core Mutations
```typescript
// convex/chat.ts
import { mutation, query } from "./_generated/server";
import { v } from "convex/values";

export const createChat = mutation({
  args: {
    name: v.string(),
    participantIds: v.array(v.id("users")),
  },
  handler: async (ctx, { name, participantIds }) => {
    return await ctx.db.insert("chats", {
      name,
      participants: participantIds,
      createdAt: Date.now(),
    });
  },
});

export const sendMessage = mutation({
  args: {
    chatId: v.id("chats"),
    content: v.string(),
    userId: v.id("users"),
  },
  handler: async (ctx, { chatId, content, userId }) => {
    // Save user message
    const userMessage = await ctx.db.insert("messages", {
      chatId,
      content,
      role: "user",
      userId,
      status: "complete",
      createdAt: Date.now(),
      updatedAt: Date.now(),
    });

    // Create AI response placeholder
    const aiMessageId = await ctx.db.insert("messages", {
      chatId,
      content: "",
      role: "assistant",
      status: "streaming",
      streamingForUser: userId,
      createdAt: Date.now(),
      updatedAt: Date.now(),
    });

    // Schedule AI response generation
    await ctx.scheduler.runAfter(0, "ai:generateResponse", {
      messageId: aiMessageId,
      chatId,
      userId,
    });

    return { userMessage, aiMessageId };
  },
});

export const getMessages = query({
  args: { chatId: v.id("chats") },
  handler: async (ctx, { chatId }) => {
    return await ctx.db
      .query("messages")
      .withIndex("by_chat", (q) => q.eq("chatId", chatId))
      .order("asc")
      .collect();
  },
});

export const updateStreamingMessage = mutation({
  args: {
    messageId: v.id("messages"),
    content: v.string(),
    status: v.optional(v.union(v.literal("streaming"), v.literal("complete"), v.literal("error"))),
  },
  handler: async (ctx, { messageId, content, status }) => {
    await ctx.db.patch(messageId, {
      content,
      updatedAt: Date.now(),
      ...(status && { status }),
    });
  },
});
```

### 3. AI Response Generation
```typescript
// convex/ai.ts
import { internalAction, internalMutation } from "./_generated/server";
import { internal } from "./_generated/api";
import { v } from "convex/values";
import { streamText } from "ai";
import { openai } from "@ai-sdk/openai";

export const generateResponse = internalAction({
  args: {
    messageId: v.id("messages"),
    chatId: v.id("chats"),
    userId: v.id("users"),
  },
  handler: async (ctx, { messageId, chatId, userId }) => {
    try {
      // Get message history
      const messages = await ctx.runQuery(internal.chat.getMessageHistory, {
        chatId,
      });

      let accumulatedContent = "";
      let lastUpdate = Date.now();
      const UPDATE_INTERVAL = 2000; // Update other users every 2 seconds

      const result = await streamText({
        model: openai("gpt-4"),
        messages: messages.map((m) => ({
          role: m.role,
          content: m.content,
        })),
        onChunk: async ({ chunk }) => {
          accumulatedContent += chunk.text || "";

          // Periodic updates for other users
          if (Date.now() - lastUpdate > UPDATE_INTERVAL) {
            await ctx.runMutation(internal.chat.updateStreamingMessage, {
              messageId,
              content: accumulatedContent,
            });
            lastUpdate = Date.now();
          }
        },
        onFinish: async (result) => {
          // Final update
          await ctx.runMutation(internal.chat.updateStreamingMessage, {
            messageId,
            content: result.text,
            status: "complete",
          });
        },
      });

      return result.text;
    } catch (error) {
      console.error("AI generation error:", error);
      await ctx.runMutation(internal.chat.updateStreamingMessage, {
        messageId,
        content: "Sorry, I encountered an error generating a response.",
        status: "error",
      });
    }
  },
});
```

### 4. HTTP Actions for Streaming
```typescript
// convex/http.ts
import { httpAction } from "./_generated/server";
import { internal } from "./_generated/api";
import { streamText } from "ai";
import { openai } from "@ai-sdk/openai";

export const chatStream = httpAction(async (ctx, request) => {
  const { messages, chatId, userId, messageId } = await request.json();

  try {
    const result = await streamText({
      model: openai("gpt-4"),
      messages,
      onFinish: async (result) => {
        // Update the message in Convex when streaming completes
        await ctx.runMutation(internal.chat.updateStreamingMessage, {
          messageId,
          content: result.text,
          status: "complete",
        });
      },
    });

    return result.toAIStreamResponse();
  } catch (error) {
    console.error("Streaming error:", error);
    return new Response("Error", { status: 500 });
  }
});
```

## Frontend Implementation

### 1. Convex Provider Setup
```typescript
// app/layout.tsx
import { ConvexProvider, ConvexReactClient } from "convex/react";

const convex = new ConvexReactClient(process.env.NEXT_PUBLIC_CONVEX_URL!);

export default function RootLayout({
  children,
}: {
  children: React.ReactNode;
}) {
  return (
    <html lang="en">
      <body>
        <ConvexProvider client={convex}>
          {children}
        </ConvexProvider>
      </body>
    </html>
  );
}
```

### 2. Chat Component
```typescript
// components/Chat.tsx
"use client";

import { useState, useEffect, useRef } from "react";
import { useChat } from "ai/react";
import { useMutation, useQuery } from "convex/react";
import { api } from "../convex/_generated/api";
import { Id } from "../convex/_generated/dataModel";

interface ChatProps {
  chatId: Id<"chats">;
  currentUserId: Id<"users">;
}

export function Chat({ chatId, currentUserId }: ChatProps) {
  const [streamingMessageId, setStreamingMessageId] = useState<Id<"messages"> | null>(null);
  const [localStreamingContent, setLocalStreamingContent] = useState("");
  const messagesEndRef = useRef<HTMLDivElement>(null);

  const messages = useQuery(api.chat.getMessages, { chatId });
  const sendMessage = useMutation(api.chat.sendMessage);

  const { input, handleInputChange, handleSubmit, isLoading } = useChat({
    api: "/api/chat",
    body: { chatId, userId: currentUserId },
    onResponse: async (response) => {
      const data = await response.json();
      setStreamingMessageId(data.aiMessageId);
    },
    onChunk: (chunk) => {
      setLocalStreamingContent((prev) => prev + (chunk.text || ""));
    },
    onFinish: () => {
      setLocalStreamingContent("");
      setStreamingMessageId(null);
    },
    onError: (error) => {
      console.error("Chat error:", error);
      setStreamingMessageId(null);
      setLocalStreamingContent("");
    },
  });

  const scrollToBottom = () => {
    messagesEndRef.current?.scrollIntoView({ behavior: "smooth" });
  };

  useEffect(() => {
    scrollToBottom();
  }, [messages, localStreamingContent]);

  const handleFormSubmit = async (e: React.FormEvent) => {
    e.preventDefault();
    if (!input.trim()) return;

    try {
      const result = await sendMessage({
        chatId,
        content: input,
        userId: currentUserId,
      });

      // Prepare messages for AI
      const chatMessages = messages?.map((m) => ({
        role: m.role,
        content: m.content,
      })) || [];

      chatMessages.push({ role: "user", content: input });

      // Trigger streaming
      handleSubmit(e, {
        body: {
          chatId,
          userId: currentUserId,
          messageId: result.aiMessageId,
          messages: chatMessages,
        },
      });
    } catch (error) {
      console.error("Send message error:", error);
    }
  };

  return (
    <div className="flex flex-col h-screen max-w-4xl mx-auto">
      {/* Messages */}
      <div className="flex-1 overflow-y-auto p-4 space-y-4">
        {messages?.map((message) => (
          <MessageBubble
            key={message._id}
            message={message}
            currentUserId={currentUserId}
            isStreaming={message._id === streamingMessageId}
            streamingContent={
              message._id === streamingMessageId ? localStreamingContent : ""
            }
          />
        ))}
        <div ref={messagesEndRef} />
      </div>

      {/* Input */}
      <div className="border-t p-4">
        <form onSubmit={handleFormSubmit} className="flex space-x-2">
          <input
            type="text"
            value={input}
            onChange={handleInputChange}
            placeholder="Type your message..."
            className="flex-1 px-3 py-2 border rounded-lg focus:outline-none focus:ring-2 focus:ring-blue-500"
            disabled={isLoading}
          />
          <button
            type="submit"
            disabled={isLoading || !input.trim()}
            className="px-4 py-2 bg-blue-500 text-white rounded-lg hover:bg-blue-600 disabled:opacity-50"
          >
            {isLoading ? "..." : "Send"}
          </button>
        </form>
      </div>
    </div>
  );
}
```

### 3. Message Bubble Component
```typescript
// components/MessageBubble.tsx
import { Id } from "../convex/_generated/dataModel";

interface Message {
  _id: Id<"messages">;
  content: string;
  role: "user" | "assistant";
  userId?: Id<"users">;
  status: "complete" | "streaming" | "error";
  streamingForUser?: Id<"users">;
}

interface MessageBubbleProps {
  message: Message;
  currentUserId: Id<"users">;
  isStreaming?: boolean;
  streamingContent?: string;
}

export function MessageBubble({
  message,
  currentUserId,
  isStreaming,
  streamingContent,
}: MessageBubbleProps) {
  const isUser = message.role === "user";
  const isMyStream = message.streamingForUser === currentUserId;

  const getDisplayContent = () => {
    if (message.status === "streaming") {
      if (isMyStream && isStreaming && streamingContent) {
        return streamingContent;
      }
      return message.content || "AI is thinking...";
    }
    return message.content;
  };

  return (
    <div className={`flex ${isUser ? "justify-end" : "justify-start"}`}>
      <div
        className={`max-w-xs lg:max-w-md px-4 py-2 rounded-lg ${
          isUser
            ? "bg-blue-500 text-white"
            : "bg-gray-200 text-gray-800"
        }`}
      >
        <p className="text-sm">{getDisplayContent()}</p>
        {message.status === "streaming" && (
          <div className="flex items-center mt-1">
            <div className="animate-pulse flex space-x-1">
              <div className="w-1 h-1 bg-current rounded-full"></div>
              <div className="w-1 h-1 bg-current rounded-full"></div>
              <div className="w-1 h-1 bg-current rounded-full"></div>
            </div>
            {!isMyStream && (
              <span className="ml-2 text-xs opacity-75">Streaming...</span>
            )}
          </div>
        )}
      </div>
    </div>
  );
}
```

### 4. API Route
```typescript
// app/api/chat/route.ts
import { NextRequest } from "next/server";
import { ConvexHttpClient } from "convex/browser";
import { api } from "../../../convex/_generated/api";

const convex = new ConvexHttpClient(process.env.CONVEX_URL!);

export async function POST(request: NextRequest) {
  try {
    const body = await request.json();
    const { messages, chatId, userId, messageId } = body;

    // Forward to Convex HTTP action
    const response = await fetch(`${process.env.CONVEX_URL}/chatStream`, {
      method: "POST",
      headers: {
        "Content-Type": "application/json",
      },
      body: JSON.stringify({
        messages,
        chatId,
        userId,
        messageId,
      }),
    });

    return response;
  } catch (error) {
    console.error("API error:", error);
    return new Response("Internal Server Error", { status: 500 });
  }
}
```

## Streaming Optimization

### 1. Efficient Update Patterns
```typescript
// convex/optimization.ts
import { internalMutation } from "./_generated/server";
import { v } from "convex/values";

export const batchUpdateMessages = internalMutation({
  args: {
    updates: v.array(
      v.object({
        messageId: v.id("messages"),
        content: v.string(),
        status: v.optional(v.string()),
      })
    ),
  },
  handler: async (ctx, { updates }) => {
    // Batch multiple updates in a single transaction
    const promises = updates.map(({ messageId, content, status }) =>
      ctx.db.patch(messageId, {
        content,
        updatedAt: Date.now(),
        ...(status && { status }),
      })
    );

    await Promise.all(promises);
  },
});
```

### 2. Cleanup Utilities
```typescript
// convex/cleanup.ts
import { internalMutation } from "./_generated/server";

export const cleanupStaleMessages = internalMutation({
  handler: async (ctx) => {
    const oneHourAgo = Date.now() - 60 * 60 * 1000;
    
    // Clean up old streaming messages that never completed
    const staleMessages = await ctx.db
      .query("messages")
      .withIndex("by_status", (q) => q.eq("status", "streaming"))
      .filter((q) => q.lt(q.field("updatedAt"), oneHourAgo))
      .collect();

    for (const message of staleMessages) {
      await ctx.db.patch(message._id, {
        status: "error",
        content: message.content || "Message failed to complete",
      });
    }
  },
});

// Schedule cleanup to run periodically
export const scheduleCleanup = internalMutation({
  handler: async (ctx) => {
    await ctx.scheduler.runAfter(60 * 60 * 1000, "cleanup:cleanupStaleMessages");
  },
});
```

## Multi-User Features

### 1. Presence System
```typescript
// convex/presence.ts
import { mutation, query } from "./_generated/server";
import { v } from "convex/values";

export const updatePresence = mutation({
  args: {
    chatId: v.id("chats"),
    userId: v.id("users"),
    status: v.union(v.literal("online"), v.literal("typing"), v.literal("away")),
  },
  handler: async (ctx, { chatId, userId, status }) => {
    const existing = await ctx.db
      .query("presence")
      .withIndex("by_user", (q) => q.eq("userId", userId))
      .filter((q) => q.eq(q.field("chatId"), chatId))
      .unique();

    if (existing) {
      await ctx.db.patch(existing._id, {
        status,
        lastSeen: Date.now(),
      });
    } else {
      await ctx.db.insert("presence", {
        chatId,
        userId,
        status,
        lastSeen: Date.now(),
      });
    }
  },
});

export const getPresence = query({
  args: { chatId: v.id("chats") },
  handler: async (ctx, { chatId }) => {
    const fiveMinutesAgo = Date.now() - 5 * 60 * 1000;
    
    return await ctx.db
      .query("presence")
      .withIndex("by_chat", (q) => q.eq("chatId", chatId))
      .filter((q) => q.gt(q.field("lastSeen"), fiveMinutesAgo))
      .collect();
  },
});
```

### 2. Typing Indicators
```typescript
// components/TypingIndicator.tsx
import { useQuery, useMutation } from "convex/react";
import { api } from "../convex/_generated/api";
import { useEffect } from "react";

interface TypingIndicatorProps {
  chatId: Id<"chats">;
  currentUserId: Id<"users">;
  isTyping: boolean;
}

export function TypingIndicator({ chatId, currentUserId, isTyping }: TypingIndicatorProps) {
  const presence = useQuery(api.presence.getPresence, { chatId });
  const updatePresence = useMutation(api.presence.updatePresence);

  useEffect(() => {
    if (isTyping) {
      updatePresence({
        chatId,
        userId: currentUserId,
        status: "typing",
      });
    } else {
      updatePresence({
        chatId,
        userId: currentUserId,
        status: "online",
      });
    }
  }, [isTyping, chatId, currentUserId, updatePresence]);

  const typingUsers = presence?.filter(
    (p) => p.status === "typing" && p.userId !== currentUserId
  );

  if (!typingUsers?.length) return null;

  return (
    <div className="text-sm text-gray-500 italic p-2">
      {typingUsers.length === 1
        ? "Someone is typing..."
        : `${typingUsers.length} people are typing...`}
    </div>
  );
}
```

## Deployment & Scaling

### 1. Environment Configuration
```bash
# Production environment variables
CONVEX_DEPLOYMENT=your-production-deployment
OPENAI_API_KEY=sk-your-production-key
NEXT_PUBLIC_CONVEX_URL=https://your-deployment.convex.cloud
```

### 2. Performance Monitoring
```typescript
// utils/performance.ts
export const logPerformance = (operation: string, startTime: number) => {
  const duration = Date.now() - startTime;
  console.log(`${operation} took ${duration}ms`);
  
  // In production, send to your monitoring service
  if (process.env.NODE_ENV === 'production') {
    // analytics.track('performance', { operation, duration });
  }
};
```

### 3. Error Handling
```typescript
// utils/error-handling.ts
export class ChatError extends Error {
  constructor(
    message: string,
    public code: string,
    public retryable: boolean = false
  ) {
    super(message);
    this.name = 'ChatError';
  }
}

export const handleChatError = (error: unknown) => {
  console.error('Chat error:', error);
  
  if (error instanceof ChatError) {
    return {
      message: error.message,
      code: error.code,
      retryable: error.retryable,
    };
  }
  
  return {
    message: 'An unexpected error occurred',
    code: 'UNKNOWN_ERROR',
    retryable: true,
  };
};
```

### 4. Scaling Considerations

**Database Optimization:**
- Index frequently queried fields
- Use compound indexes for complex queries
- Implement data archiving for old messages

**Caching Strategy:**
- Leverage Convex's built-in caching
- Cache user sessions and presence data
- Implement client-side caching for static data

**Rate Limiting:**
```typescript
// convex/rate-limiting.ts
import { mutation } from "./_generated/server";
import { v } from "convex/values";

const RATE_LIMIT = 10; // messages per minute
const WINDOW_MS = 60 * 1000;

export const rateLimitedSendMessage = mutation({
  args: {
    chatId: v.id("chats"),
    content: v.string(),
    userId: v.id("users"),
  },
  handler: async (ctx, args) => {
    const now = Date.now();
    const windowStart = now - WINDOW_MS;
    
    const recentMessages = await ctx.db
      .query("messages")
      .withIndex("by_chat", (q) => q.eq("chatId", args.chatId))
      .filter((q) => 
        q.and(
          q.eq(q.field("userId"), args.userId),
          q.gt(q.field("createdAt"), windowStart)
        )
      )
      .collect();
    
    if (recentMessages.length >= RATE_LIMIT) {
      throw new Error("Rate limit exceeded");
    }
    
    // Proceed with normal message sending
    return await ctx.runMutation("chat:sendMessage", args);
  },
});
```

## Summary

This implementation provides:
- **Efficient Streaming**: Real-time for active users, periodic updates for others
- **Multi-User Support**: Presence, typing indicators, and synchronized state
- **Scalable Architecture**: Optimized database operations and error handling
- **Production Ready**: Rate limiting, monitoring, and cleanup utilities

The key insight is balancing immediate streaming UX with efficient multi-user synchronization - giving the active user real-time streaming while keeping others informed through Convex's reactive system without overwhelming the database.